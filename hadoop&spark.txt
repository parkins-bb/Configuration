http://dblab.xmu.edu.cn/blog/1177-2/ -Hadoop 2.7分布式集群环境搭建
http://wuchong.me/blog/2015/04/04/spark-on-yarn-cluster-deploy/ -spark on yarn 集群安装搭建
http://dblab.xmu.edu.cn/blog/2081-2/ -在阿里云ECS的Ubuntu中安装Spark
---------------------------------------------------------------
Spark集群搭建过程
scala
spark
解压在：/opt/spark  /opt/sacla
---------------------------------------------------------------
vi/etc/profile
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/
export JRE_HOME=$JAVA_HOME/jre
export HADOOP_HOME=/opt/hadoop-2.7.7
export SCALA_HOME=/opt/scala
export SPARK_HOME=/opt/spark
export CLASSPATH=$JAVA_HOME/lib:$JRE_HOME/lib:$CLASSPATH
export PATH=$JAVA_HOME/bin:$JRE_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$SCALA_HOME/bin:$SPARK_HOME/bin:$SPARK_HOME/sbin:$PATH
----------------------------------------------------------------
spark 相关配置:
cd /opt/spark/conf
cp slaves.template slaves
cp spark-env.sh.template spark-env.sh
cp log4j.properties.template log4j.properties
-------------------------------------------
vi log4j.properties
将log4j.rootCategory = INFO,console 改为 og4j.rootCategory = WARN,console
------------------------------------------------------
vi slaves 替换localhost
master
slave1
slave2
slave3
-------------------------------------
vi spark-env.sh
export SPARK_DIST_CLASSPATH=$(/opt/hadoop-2.7.7/bin/hadoop classpath)
export HADOOP_CONF_DIR=/opt/hadoop-2.7.7/etc/hadoop
export SPARK_LOCAL_IP=0.0.0.0
export SPARK_MASTER_HOST=master
--------------------------------------
scp slaves slave3:/opt/spark/conf
scp spark-env.sh slave3:/opt/spark/conf
--------------------------------------
注意：
spark目录sbin中有stop-all.sh
hadoop目录sbin中有stop-all.sh
------------------------------------
前提是hadoop已经启动运行
启动集群：如果配置路径$SPARK_HOME/sbin:$PATH
输入：
start-master.sh  
start-salves.sh
如果没有配置：
则进入/opt/spark/sbin 目录下：
start-master.sh  
start-salves.sh
-----------------------------------------
关闭集群：先关闭spark 在关闭hadoop
关闭spark：
关闭master节点
stop-master.sh
关闭worker节点
stop-slaves.sh
关闭hadoop：
stop-all.sh
----------------------------------
配置不成功需要关闭集群将spark目录中的logs清理干净。


--------------------------------
anaconda 配置 pyspark
--------------------------------
vi ~/.bashrc 
export PATH=/root/anaconda3/bin:$PATH
export ANACONDA_PATH=/root/anaconda3
export PYSPARK_DRIVER_PYTHON=$ANACONDA_PATH/bin/ipython
export PYSPARK_PYTHON=$ANACONDA_PATH/bin/python
--------------------------
启动：pyspark
PYSPARK_DRIVER_PYTHON=ipython PYSPARK_DRIVER_PYTHON_OPTS="notebook" pyspark


